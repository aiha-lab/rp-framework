# Model
model_name_or_path: /raid/LLM/llama3.2-1b-instruct-sft
attn_implementation: eager
torch_dtype: bfloat16

# Dataset
dataset_name: dataset/piqa-train-llama3.2
dataset_num_proc: 12

# Bit-precision (Linear)
w_format: fp4_e2m1
a_format: fp4_e2m1
g_format: null

# Hyperparameters
learning_rate: 2.0e-5
gradient_checkpointing: true
num_train_epochs: 1.0
logging_steps: 1
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 2
max_length: 2048
warmup_ratio: 0.03
lr_scheduler_type: cosine_with_min_lr
lr_scheduler_kwargs:
  min_lr_rate: 0.1
output_dir: /raid/LLM/llama3.2-1b-instruct-sft-qat-w4a4
report_to: []
seed: 42
eval_on_start: false
eval_strategy: steps
eval_steps: 300
dataset_test_split: test
